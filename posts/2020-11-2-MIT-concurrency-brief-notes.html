<blurb>MIT concurrency notes</blurb>

<div id="toc_container">
  <p class="toc_title">Contents</p>
  <ul>
    <li>
      <a href="#lecture_19">Lecture 19: Peer-to-peer: Bitcoin</a>
    </li>
    <li>
      <a href="#lecture_18"
        >Lecture 18: Fork Consistency, Certificate Transparency</a
      >
    </li>
    <li>
      <a href="#lecture_17">Lecture 17: Causal Consistency, COPS</a>
    </li>
    <li>
      <a href="#lecture_16"
        >Lecture 16: Cache Consistency: Memcached at Facebook</a
      >
    </li>
    <li><a href="#lecture_15">Lecture 15: Big Data: Spark</a></li>
    <li>
      <a href="#lecture_14">Lecture 14: Optimistic Concurrency Control</a>
    </li>
    <li><a href="#lecture_13">Lecture 13: Spanner</a></li>
    <li><a href="#lecture_12">Lecture 12: Distributed Transactions</a></li>
    <li><a href="#lecture_11">Lecture 11: Cache Consistency: Frangipani</a></li>
    <li><a href="#lecture_10">Lecture 10: Cloud Replicated DB, Aurora</a></li>
    <li><a href="#lecture_9">Lecture 9: More Replication, CRAQ</a></li>
    <li><a href="#lecture_8">Lecture 8: Zookeeper</a></li>
    <li><a href="#lecture_6_7">Lecture 6-7: Fault Tolerance: Raft</a></li>
    <li><a href="#lecture_5">Lecture 5: Go, Threads, and Raft</a></li>
    <li><a href="#lecture_4">Lecture 4: Primary-Backup Replication</a></li>
    <li><a href="#lecture_3">Lecture 3: GFS</a></li>
    <li><a href="#lecture_2">Lecture 2: RPC and Threads</a></li>
    <li><a href="#lecture_1">Lecture 1: Introduction</a></li>
  </ul>
</div>

<p id="lecture_19" class="lecture_heading">Lecture 19: Peer-to-peer: Bitcoin</p>
<ul class="note_bullets">
  <li>
    Paper:
    <a href="https://pdos.csail.mit.edu/6.824/papers/bitcoin.pdf"
      >Bitcoin (2008)</a
    >;
    <a href="https://www.youtube.com/watch?v=K_euhRou98Y">Lecture</a>
  </li>
  <li>Peer to peer system to establish trust by making a chain of hashes</li>
  <li>
    <ul>
      <li>
        Simplest solution is to have a single trusted entity. But not great
        because no one entity is trusted by everyone globally
      </li>
      <li>
        Alternatively, use votes by peers. Has issues where someone can make
        many fake peers and vote for themselves
      </li>
    </ul>
  </li>
  <li>
    Bitcoin
    <ul>
      <li>
        Prove work by generating hashes with a given number of zeros. Miners
        rewarded with bitcoin
      </li>
      <li>
        If most of the cpu power is controlled by non-malicious actor, should
        generally create the next block first
      </li>
      <li>
        Handles forks (two successors to a single block) when they arise by
        having all miners switch to longer forks as soon as they learn of it
      </li>
      <li>
        Its possible in short forks for payments to be overwritten. The solution
        is to wait for many blocks to appear after the transacation you are
        interested in
      </li>
      <li>
        Determining the difficulty of the hardness is done deterministically
        based on the timestamps of the blockchain. Forces agreement as everyone
        sees the same chain
      </li>
    </ul>
  </li>
  <li>
    Limitations:
    <ul>
      <li>
        Blocks are limited in size, which highly limits the transactions per
        second
      </li>
      <li>
        Interblock interval hard to make shorter as making it significantly
        shorter makes it harder for miners to learn of updated peers and thus
        they would be wasting much of their compute time
      </li>
      <li>Compute time is costly</li>
    </ul>
  </li>
</ul>

<p id="lecture_18" class="lecture_heading">
  Lecture 18: Fork Consistency, Certificate Transparency
</p>
<ul class="note_bullets">
  <li>
    Paper:
    <a href="https://research.swtch.com/tlog">Certificate Transparency</a>;
    <a href="https://www.youtube.com/watch?v=UKdLJ7-0iFM">Lecture</a>
  </li>
  <li>
    System to trust an append only log that is potentially run by a bad agent
  </li>
  <li>
    The original idea about SSL/TLS was to have certificate authorities be able
    to check that everyone who claims to own a domain/DNS actually does.
    However, now there are too many websites and there are too many certificate
    authorities
    <ul>
      <li>
        As a result, sometimes the certificate authorities make a mistake or go
        rogue and issue bad certs that allow for man in the middle attacks
      </li>
      <li>Very difficult to detect what certs are good and what are fake</li>
    </ul>
  </li>
  <li>
    Certificate Transparency
    <ul>
      <li>
        Allow auditing of validity of certificates by revealing all information
        publicly
      </li>
      <li>
        Certificate authorities make certs for servers and also sends a copy to
        the certificate transparency log server
      </li>
      <li>
        Client browsers validate certificates are in the certificate
        transparency log servers
      </li>
      <li>
        No browser will use certs a log not in the log (could stil be bad certs
        issued by certificate authorities)
      </li>
      <li>
        Each website builds a "monitor" service to check for any rogue
        certificates
      </li>
      <li>
        Append only (to prevent deletion by rogue users), no forks (no
        equivocation aka don't show two different logs to two different users),
        and ability to handle when untrusted
      </li>
    </ul>
  </li>
  <li>
    Merkle tree
    <ul>
      <li>
        Pairs of numbers are hashed in a tree structure. Tree heads are signed
        so that log servers cannot disavow them later
      </li>
      <li>
        Browsers on getting a cert ask log servers for a proof of inclusion
        <ul>
          <li>Asks for the signed tree head</li>
          <li>
            Then asks for the position of the given certificate + hashes of the
            sibling nodes to the parent. Computes the hashs locally and compares
            vs signed tree head. Computationally infeasible to produce a fake
            hash
          </li>
          <li>Only has log(n) sibling hashes. Makes this fast</li>
        </ul>
      </li>
      <li>
        Once forked, must maintain the fork indefinitely. Can be defeated by
        gossip between various clients
        <ul>
          <li>Figure out of one log is a prefix of another</li>
          <li>
            Given two signed tree heads, ask proof from log server that one is a
            prefix of another. Log server sends sibings of the nodes to the more
            recent tree head and then client can compute the hashes locally to
            compare
          </li>
          <li>
            Browser challenges log servers with previous signed tree heads
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p id="lecture_17" class="lecture_heading">
  Lecture 17: Causal Consistency, COPS
</p>
<ul class="note_bullets">
  <li>
    Paper:
    <a href="https://pdos.csail.mit.edu/6.824/papers/cops.pdf">COPS (2011)</a>;
    <a href="https://www.youtube.com/watch?v=fR_NB714EAI">Lecture</a>
  </li>
  <li>
    Scaling a consistency model while using a consistency model based on
    causation of events
  </li>
  <li>
    Goals
    <ul>
      <li>Local reads + local writes</li>
      <li>
        Easiest way is to use
        <i>eventual consistency</i> (See Dynamo and Cassandra)
        <ul>
          <li>Reads occur locally</li>
          <li>
            Writes occur locally, but each writer has a write queue of items
            that they send to replicated servers
          </li>
          <li>
            Other readers on other servers are not guaranteed to see your write.
            They will eventually, but the order might be different
          </li>
          <li>
            Difficult to use as an app programmer as related updates might
            return out of order. Ex. Upload Photo → Add to list. The add to list
            might occur before the photo upload replication happens and thus
            have be an empty pointer
          </li>
        </ul>
      </li>
      <li>
        Eventual Consistency has issues with reconcilling simultaneous writes to
        the same key
        <ul>
          <li>
            Wall clock to break ties is naive solution (same time ties can be
            broken with data center id for example), but there can be drift,
            which results in many writes being rejected incorrectly
          </li>
          <li>
            Lamport (logical) clocks
            <ul>
              Every server keeps a value of Tmax, which is the higher number it
              has seen anywhere else
              <li>
                When server assigns a timestamp, it takes max(Tmax + 1, real
                time)
              </li>
            </ul>
          </li>
          <li>Last writer wins policy is easy</li>
          <li>
            Struggles with atomic increments (transactions/mini-transacations)
          </li>
        </ul>
      </li>
      <li>
        Eventual Consistency + Barrier
        <ul>
          <li>
            Implements
            <i>sync</i>
            operation, which forces a wait on a key until version is at least a
            given value
          </li>
        </ul>
      </li>
      <li>
        Use a single log server at local datacenter
        <ul>
          <li>
            Orders all local operations and makes sure they arrive at other
            servers in the right order. Unfortunately, this becomes a potential
            bottleneck
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    COPS + Causal Consistency
    <ul>
      <li>
        Client Context
        <ul>
          <li>
            Keeps track of operations a client does on the client side to
            understand ordering dependencies. The client keeps track of version
            numbers obtained by various operations
          </li>
          <li>
            EX (where v indicate version number returned): get(x): v2 → get(y):
            v4 → put(z, value, {x: v2, y: v4}): v3
          </li>
        </ul>
      </li>
      <li>
        Local server gets client operations and forwards data + dependencies to
        replicas. Replicas must then wait until the dependencies are applied
        before being able to apply the write. Replicas query other servers in
        its datacenter to figure out the version numbers of the depenendencies
        and if they are high enough to apply operation
      </li>
      <li>
        Any put allows for for subsequent operations to use only the put as its
        context instead of all proceeding operations. This is correct because
        the put implicitly has all its proceeding operations as a dependency
      </li>
      <li>Performance not notable, 50k ops/second</li>
      <li>
        Active area of research (OCCULT for example improves on COPS by not
        using explicit dependencies and using "causal" timestamps, which is
        based on real time, that indicate how far advanced a single shard is. If
        timestamps returned on a read > local, need to retry or query master)
      </li>
    </ul>
  </li>
  <li>
    COPS with Get Transactions
    <ul>
      <li>
        Response to Get Transaction gives the version as well as all the
        dependencies of each value
      </li>
      <li>
        For each item returned, check its dependencies and make sure that the
        item returned from the Get Transacation has at least the version in the
        dependency
      </li>
      <li>
        If it doesn't, send a second round of gets. This doesn't introduce
        additional waiting for dependencies as the initial Get Transaction
        having a certain value implies that all its dependencies are already
        updated to the required version. Two rounds of gets to make sure all
        dependencies are the correct version
      </li>
    </ul>
  </li>
</ul>

<p id="lecture_16" class="lecture_heading">
  Lecture 16: Cache Consistency: Memcached at Facebook
</p>
<ul class="note_bullets">
  <li>
    Paper:
    <a href="https://pdos.csail.mit.edu/6.824/papers/memcache-fb.pdf"
      >Memcached at Facebook (2013)</a
    >; <a href="https://www.youtube.com/watch?v=Myp8z0ybdzM">Lecture</a>
  </li>
  <li>Handling issues while scaling memcached</li>
  <li>
    Standard read:
    <ul>
      <li>
        Read: try get from memcached → if return null → fetch from DB → stored
        returned value in memcached
      </li>
      <li>
        Write: send new value to DB → delete from memcached (invalidate scheme)
      </li>
      <li>
        Could alternatively, send updated data to memcached on write instead of
        deleting from memcached (harder to get right)
      </li>
      <li>
        Because all writes go through the primary region, a write stemming from
        the secondary region might not get propagated to the secondary region's
        database before the cache is invalidated. Solved by adding a key as
        "remote" which tells future clients to look at the primary region when
        failing to get a hit in memcached
      </li>
    </ul>
  </li>
  <li>
    Performance
    <ul>
      <li>
        Partition
        <ul>
          <li>Keep one copy of each key means its RAM efficient</li>
          <li>Not good for hot, very frequently accessed keys</li>
          <li>Clients talk to every partition</li>
        </ul>
      </li>
      <li>
        Replication
        <ul>
          <li>Good when handling very hot keys</li>
          <li>Clients talk to one server only</li>
          <li>Handle a lot less total data</li>
        </ul>
      </li>
      <li>
        Starting new memcached cluster is slow
        <ul>
          <li>
            On miss for "cold" clusters, tries local memcached first, but then
            tries a different cluster. If it finds in a hot cluster, will add to
            the cold cluster otherwise query database
          </li>
          <li>
            Prevent inconsistency with database updates by adding a non-zero
            "hold" time to add operations after a delete
          </li>
        </ul>
      </li>
      <li>
        Prevent thundering herd after cache invalidation by using a "lease".
        <ul>
          <li>First client to get a miss will get a lease # from memcached</li>
          <li>
            Other front ends getting a miss will be told to wait because
            memcached has a lease #
          </li>
          <li>Is a lease bcause eventually it will time out</li>
          <li>
            Makes it so that only a single client actually queries the database
          </li>
          <li>
            Lease mechanism also used to prevent out of order writes after a
            delete (cache invalidation). Cache invalidation invalidates any
            outstanding leases so writer cannot write with stale data
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    Fault Tolerance with Gutter servers
    <ul>
      <li>Idle unless main memcached server is dead</li>
      <li>
        Client on memcached failure due to being unable to contact a server,
        contacts gutter server
      </li>
      <li>
        Key doesn't exist in gutter server means client reads from DB and then
        installs in gutter servers
      </li>
      <li>
        Keys are not explicitly deleted because otherwise excessive number of
        delete requests to it as gutter can store any key and might not be
        partitioned. Instead entries expire quickly
      </li>
    </ul>
  </li>
</ul>

<p id="lecture_15" class="lecture_heading">Lecture 15: Big Data: Spark</p>
<ul class="note_bullets">
  <li>
    Paper:
    <a href="https://pdos.csail.mit.edu/6.824/papers/zaharia-spark.pdf"
      >Spark (2012)</a
    >; <a href="https://www.youtube.com/watch?v=mzIoSW-cInA">Lecture</a>
  </li>
  <li>
    Generalizes two stages of MapReduce into multi-step dataflow graphs. Not
    great at streaming data
  </li>
  <li>
    Build lazy lineage graph instead of actually computing. Lazy computing
    <ul>
      <li>
        Only executed when running an <i>action</i> (count, collect, reduce,
        lookup, save)
      </li>
      <li>
        Something like a distinct requires a distributed sort/sending info to
        right workers or hashing to figure out how to send info to right workers
      </li>
      <li>Persist/cache → stores data in memory so don't have to recompute</li>
    </ul>
  </li>
  <li>
    Data is directly piped in memory to next step, unlike in MapReduce which
    must create intermediate files. More efficient
    <ul>
      <li>
        For Narrow dependencies (only requires local data), no network
        interaction at all
      </li>
      <li>
        Last narrow stage will put output to disk and then for the wide stage
        (dependent on multiple previous stages), it reads the buckets of the
        file. Similar to MapReduce's reduce step
      </li>
      <li>
        By generating lineage graph can optimize wide operations by avoiding
        unnecessary operations
      </li>
    </ul>
  </li>
  <li>
    Fault Tolerance
    <ul>
      <li>
        Fault tolerance by just rerunning the lineage graph as operations are
        immutable
      </li>
      <li>
        Works well for narrow dependencies, but wide dependencies require
        potentially data from all other workers, if not saved to disk might need
        to rerun all those computations
      </li>
      <li>Use snapshotting to avoid having to rerun all computations</li>
      <li>
        Very large clusters so need to have a reasonable failure recovery system
      </li>
    </ul>
  </li>
</ul>

<p id="lecture_14" class="lecture_heading">
  Lecture 14: Optimistic Concurrency Control
</p>
<ul class="note_bullets">
  <li>
    Paper:
    <a href="https://pdos.csail.mit.edu/6.824/papers/farm-2015.pdf"
      >FARM (2015)</a
    >; <a href="https://www.youtube.com/watch?v=Cw6Nj2evjSs">Lecture</a>
  </li>
  <li>
    Use remote direct memory access + non volatile memory (battery powered) to
    provide serializability (ordered transactions)
  </li>
  <li>
    Research prototype where the app uses special network interface cards (NICs)
    to read from memory of app on a different computer directly. Only viable in
    local networks at the moment, but crazy good performance (100x faster than
    spanner). Uses two phase commits
  </li>
  <li>
    Optimistic Concurrency Control by first reading the value directly from app
    memory on a different computer and then validating during the transaction
    after acquiring locks. Pessimistic Concurrency Control blocks on acquring
    locks while Optimistic aborts when it find that the valid it has read is no
    longer valid.
  </li>
  <li>Direct Read → Lock → Validate → Commit</li>
</ul>

<p id="lecture_13" class="lecture_heading">Lecture 13: Spanner</p>
<ul class="note_bullets">
  <li>
    Paper:
    <a href="https://pdos.csail.mit.edu/6.824/papers/spanner.pdf"
      >Spanner (2012)</a
    >; <a href="https://www.youtube.com/watch?v=4eW5SWBi7vs">Lecture</a>
  </li>
  <li>
    Use Paxos to make the transacation coordinator in two phase commits have
    fallbacks and so it does not completely block on transacation coordinator
    failure (availability). This allows for external
    consistency/serializability. High demand by programmers for transacations as
    easier to reason about
  </li>
  <li>
    Use versioning of objects (snapshot isolation) + time API to allow many read
    only transactions to complete by reading from the the local instance without
    locking. Paxos sends operations in timestamp order. Must wait until sees a
    Paxos operation with timestamp after the timestamp of the transaction (See
    start rule)
  </li>
  <li>
    Time API has uncertainty.
    <ul>
      <li>
        Start Rule: Transaction timestamp = time.now.latest.
        <ul>
          <li>Read only: start time</li>
          <li>Read write: commit time</li>
        </ul>
      </li>
      <li>
        Commit Wait: Wait to commit a read write transaction until Transaction
        timestamp < time.now.earliest
      </li>
    </ul>
  </li>
  <li>Used commerially by Google and also in Cockroachdb</li>
</ul>

<p id="lecture_12" class="lecture_heading">
  Lecture 12: Distributed Transactions
</p>
<ul class="note_bullets">
  <li>
    Paper:
    <a
      href="https://ocw.mit.edu/resources/res-6-004-principles-of-computer-system-design-an-introduction-spring-2009/online-textbook/"
    >
      Read 6.033 Chapter 9, just 9.1.5, 9.1.6, 9.5.2, 9.5.3, 9.6.3 </a
    >; <a href="https://www.youtube.com/watch?v=aDp99WDIM_4">Lecture</a>
  </li>
  <li>
    <b>Serializable</b>
    <ul>
      <li>
        There exists some serial order (one at a time) of execution of the
        transactions such that yields the same result as the actual execution
      </li>
      <li>
        Given two transactions T1, T2. The only possible serial orders are: T1,
        T2 or T2, T1. A serializable system must have the same result as running
        either T1 then T2 or T2 then T1.
      </li>
    </ul>
  </li>
  <li>
    Distributed Transactions with ACID:
    <ul>
      <li>
        concurrency control (to provide isolation/serializability)
        <ul>
          <li>
            Pessimistic: Acquire lock before using data. Wait if lock held by
            something else (faster when frequent conflicts)
          </li>
          <li>
            Optimistic: Don't acquire locks and just read/write to temporary
            area. At the end check if another system is using the data at the
            same time. If it was being used, abort. Faster when conflicts are
            rare
          </li>
        </ul>
      </li>
      <li>atomic commit (to provide atomicity despite failure)</li>
    </ul>
  </li>
  <li>
    Two phase locking (Pessimistic)
    <ul>
      <li>Acquire lock before using record</li>
      <li>
        Hold lock until completely done/abort (needed as opposed to release
        immediately after last usage of a record because an abort after that
        could result in the need to rollback the entire transaction)
      </li>
      <li>
        Easy to get deadlocks (db needs to smart to detect and abort
        transactions)
      </li>
    </ul>
  </li>
  <li>
    Two phase commits (used in many databases that are sharded + have
    multi-record transactions):
    <ul>
      <li>
        Phase One: Coordinator tells all participants the operations they need
        to perform. Participants prepare locks/rollback resources as needed.
        After receiving the response and about ready to commit, coordinator asks
        all participants if they can commit. If participants says yes, they must
        be remain in a state indefinitely where they can either do the operation
        or rollback.
      </li>
      <li>
        Phase Two:
        <ul>
          <li>
            Coordinator receives yes from all participants → Sends message to
            all participants to commit repeatedly until gets acknowledgement
            from all participants.
          </li>
          <li>
            Coordinator receives no from any participants → sends abort
            repeatedly to all participants to rollback until gets
            acknowledgement from all participants.
          </li>
        </ul>
      </li>
      <li>
        Has issues when coordinator fails as participants cannot change their
        locked without coordinator command to either proceed or abort. Generally
        requires manual intervention
      </li>
    </ul>
  </li>
</ul>

<p id="lecture_11" class="lecture_heading">
  Lecture 11: Cache Consistency: Frangipani
</p>
<ul class="note_bullets">
  <li>
    Paper:
    <a href="https://pdos.csail.mit.edu/6.824/papers/thekkath-frangipani.pdf"
      >Frangipani</a
    >; <a href="https://www.youtube.com/watch?v=-pKNCjUhPjQ">Lecture</a>
  </li>
  <li>
    Networked file system with locking to guarantee cache coherence/atomicity
    using Petal (block storage service)
  </li>
  <li>
    Cache Coherence Rules
    <ul>
      <li>
        No workstation can hold cached data without holding a lock, but servers
        don't give up lock after they are done using the data to optimize for
        the typical case where one creates files and immediately use it
        <ul>
          <li>Acquire Lock by requesting from Petal → Read from Petal</li>
          <li>
            When planning to release lock → first write cached data to Petal
          </li>
        </ul>
      </li>
      <li>
        Data is typically in RAM, but anything in cache is written to file every
        ~30 seconds
      </li>
      <li>
        Lock server keeps track of locks for specific files and the workstation
        that owns that lock
      </li>
      <li>
        Example:
        <ul>
          <li>
            Workstation 1: Request lock to lock server → receive Grant lock from
            lock server → uses files but continues holding lock even when not
            using it directly
          </li>
          <li>Workstation 2: Sends a Request lock to lock server</li>
          <li>
            Lock Server: Checks table → lock owned by Workstation 1 → sends
            Revoke to workstation 1
          </li>
          <li>
            Workstation 1: Writes any cached data to write ahead log and then
            Petal → Release lock message to Lock Server
          </li>
          <li>
            Lock Server: receives Release lock from Workstation 1 → Sends Grant
            lock to Workstation 2 and Workstation 2 can use the file
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    Crash Recovery
    <ul>
      <li>
        A workstation must write to its own write ahead log in Petal (allows
        other workstations to acccess it in case of crashes) before beginning
        any write that it does. Log Entries have version number with version
        before write + 1
      </li>
      <li>
        On crash, other computer looks at log and finds the last entry via the
        monotonically increasing log sequence number. Each entry has description
        of file operations
        <ul>
          <li>
            Replays any complete log entries (no partial writes) into Petal and
            then tells Petal to release the locks
          </li>
          <li>
            Ignores log entries that have version numbers ≤ version number in
            Petal (only replays newer entries)
          </li>
          <li>
            Recovery reads without acquiring locks even though other systems may
            have the locks. (Required because if power outage knowledge about
            locks is completely lost and recovery software must continue). This
            is okay because there are only two possibilities:
            <ul>
              <li>
                workstation had lock when crashed → nothing else could have
                written to it as lock was not acquired by any other system so we
                are safe. version number < version in log
              </li>
              <li>
                workstation gave up lock when crashed → must have already
                written to Petal so version number will be ≥ version in log and
                recovery does nothing
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p id="lecture_10" class="lecture_heading">
  Lecture 10: Cloud Replicated DB, Aurora
</p>
<ul class="note_bullets">
  <li>
    Paper:
    <a href="https://pdos.csail.mit.edu/6.824/papers/aurora.pdf"
      >Aurora (2017)</a
    >; <a href="https://www.youtube.com/watch?v=jJSh54J1s5o">Lecture</a>
  </li>
  <li>
    Cloud based database on AWS with quorum based reads/writes across multiple
    availability zones
  </li>
  <li>
    Quorums
    <ul>
      <li>
        Consistency achived when <i>Vr + Vw > V</i>, where <i>Vr</i> is the
        number of votes needed for a read request, <i>Vw</i> is the number of
        votes needed for a write request and <i>V</i> is the number of copies
      </li>
      <li>
        Aurora has a goal of having read availability when one availability zone
        + 1 server is down. They achieve this by having 6 copies across 3
        availability zones. Then a single zone + 1 server can be down and a read
        quorum of 3 can be achieved. Write quorum must then be 4
      </li>
    </ul>
  </li>
  <li>
    Only stores "redo" logs as opposed to pages. Materializes pages when needed.
    The log is the database
    <ul>
      <li>
        Writes through a single database, many read only databases
        <ul>
          <li>
            Read only databases can directly interact with storage servers to
            grab data. Also cache data, but lag to some extent
          </li>
          <li>
            mini transactions/VDL numbers etc are used to ensure consistency for
            the read only databases
          </li>
        </ul>
      </li>
      <li>
        Aurora typically does not use quorum reads as it keeps track of how far
        in the log each storage server has reached and sends read requests to a
        storage server whose log is up to date enough. Quorum reads used in
        recovery
        <ul>
          <li>
            Quorum reads are used in recovery to find the first missing log
            entry amongst the quorumed servers and discards all entries
            following that
          </li>
          <li>
            Only log entries of uncommitted entries are discarded due to the
            quorum write property
          </li>
        </ul>
      </li>
      <li>
        Redo log:
        <ul>
          <li>Old value + new value</li>
          <li>
            Commit record (existence of this indicates if the transaction has
            completed or not)
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    Network is the main bottleneck, dealt with via customized storage servers
  </li>
</ul>

<p id="lecture_9" class="lecture_heading">Lecture 9: More Replication, CRAQ</p>
<ul class="note_bullets">
  <li>
    Paper:
    <a href="https://pdos.csail.mit.edu/6.824/papers/craq.pdf">CRAQ (2009)</a>;
    <a href="https://www.youtube.com/watch?v=IXHzbCuADt0">Lecture</a> (majority
    of the lecture is about Zookeeper, CRAQ part starts at ~57:20)
  </li>
  <li>
    Improves on Chain Replication with higher read throughput that allows gives
    applications the option for weaker consistency guarantees on non-committed
    operations
  </li>
  <li>
    Chain Replication
    <ul>
      <li>
        Linearly ordered sequence of servers where all writes are processed by
        head node and all reads are processed by tail node. Has strong
        consistency
      </li>
      <li>
        Head node sends data to next node, which then sends data to the node
        after it. Repeat until reaches tail node
      </li>
      <li>
        Tail node sends acknowledgement that it has committed, which then
        propagates via the chain back to the head. Then head can then respond to
        the client that the write operation has completed
      </li>
    </ul>
  </li>
  <li>
    Chain Replication with Apportioned Queries (CRAQ)
    <ul>
      <li>
        Each node stores multiple versions of an object
        <ul>
          <li>monotonically increasing version number + clean/dirty flag</li>
        </ul>
      </li>
      <li>
        When a node receives an update from a node higher up in the chain (or
        from client), appends that update to the object with an increased
        version number
        <ul>
          <li>
            Not last node → mark as dirty and propagate to node proceeding
            itself
          </li>
          <li>
            Last node → mark as clean and sends acknowledgement up the chain
          </li>
        </ul>
      </li>
      <li>
        node receives an acknowledgement message for an object → mark object as
        clean and can delete non-latest version of object
      </li>
      <li>
        node receives a read request for an object
        <ul>
          <li>
            Latest version is clean (aka has one version of the object) →
            responds with the object. The tail is guaranteed not to have seen
            the object yet
          </li>
          <li>
            Latest version is dirty → sends message to tail to get the tail's
            last commit number of the object and responds with that version of
            the object (which must exist)
          </li>
          <li>
            It is possible tail commits a new version between when it respond to
            the node and when the node sends to the client. But this is still
            strongly consistent because consistency is ordered to when tail
            responded
          </li>
        </ul>
      </li>
      <li>
        Can use as eventual consistency system by having nodes respond to read
        requests by serving the latest version available. A request to a
        different node may result in an older version of the object
      </li>
    </ul>
  </li>
  <li>
    Vs Raft/Paxos
    <ul>
      <li>
        Faster than a system like Raft/Paxos as reads/writes are divided amongst
        multiple servers, but handles lagging servers poorly
      </li>
      <li>
        Failure recovery is similar to Chain Replication in that every node
        knows its predecessor and successor and can generally replace it with no
        extra work (nodes internal in the chain failing may require resends)
      </li>
      <li>
        Not resistant to partition failure. General way to deal with this is a
        configuration manager such as Zookeeper (or similar Raft/Paxos
        replicated system) to keep track of which servers are alive and the
        current setup of the chain
      </li>
    </ul>
  </li>
</ul>

<p id="lecture_8" class="lecture_heading">Lecture 8: Zookeeper</p>
<ul class="note_bullets">
  <li>
    Paper:
    <a href="https://pdos.csail.mit.edu/6.824/papers/zookeeper.pdf"
      >ZooKeeper (2010)</a
    >;
    <a href="https://www.youtube.com/watch?v=pbmyrNjzdDk">Lecture Part1</a>
    <a href="https://www.youtube.com/watch?v=IXHzbCuADt0">Lecture Part2</a>
    (until ~57:20)
  </li>
  <li>
    Stand alone service used for coordination of other services. Uses ZAB, which
    is similar to Raft/Paxos
  </li>
  <li>
    Linearizability
    <ul>
      <li>
        A history is linearizable if there exists a total order of operations
        that matches real time and reads see proceeding writes in the order
      </li>
      <li>Cycles =/= not linearizable</li>
      <li>Zookeeper does not use the standard definition of linearizability</li>
      <li>
        Zookeeper alows reads to return stale data (unless using <i>sync</i>)
      </li>
    </ul>
  </li>
  <li>
    Zookeeper Guarantees
    <ul>
      <li>Writes are linearizable</li>
      <li>
        For any given client, Zookeeper executes operations in a client
        specified order (FIFO)
        <ul>
          <li>
            Writes: Client gives an ordering of the writes for the leader to
            execute
          </li>
          <li>
            Reads: A read has a given effective location in the log. Any future
            read must be at least as up to date as that previous read. This
            guarantee holds even when switching to a different replica
            <ul>
              <li>
                When replica responds to a client read request, replica attaches
                zxID of the previous entry to the response. Client remembers the
                most recent zxID it sees. Client sends that id to future
                requests
              </li>
              <li>Applies to reads followed by writes. Read your own writes</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <i>sync</i> + FIFO client order allows one to see the most recent data
        if needed
      </li>
    </ul>
  </li>
  <li>
    Strategies
    <ul>
      <li>
        Ready file (acts like a lock): before modifying configuration, delete
        "ready" file. Then make changes and at the end recreate "ready" file.
        This allows other systems to know that the configuration is in a good
        state
      </li>
      <li>
        When trying to read configurations, read for "ready" file existence (to
        know the configuration is good) with a watch to be notified of any
        change (and thus know the configuration is stale). Watches not carried
        over by replicate failure so need to restart with watches when replicate
        crashes (client is notified of this)
      </li>
      <li>
        Most operations have version number so zookeeper only does an operation
        when the current version number = version sent via request
      </li>
      <li>
        while true: getData(id) → if setData(id, new_value ,version + 1): break
        <ul>
          <li>mini-transaction</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    Thundering herd
    <ul>
      <li>
        Because typically one uses version numbers to prevent running with stale
        data, it is common to implment with a loop. Many many clients trying at
        the same time will result in a thundering herd
        <ul>
          <li>
            General solution: Randomized Exponentially increasing delays between
            retries
          </li>
          <li>
            Ephemeral nodes: Tries to create ephemeral, but if it fails check if
            exists with a watch to know when the lock is released
          </li>
          <li>
            Sequential file: Check if previously number file in the sequence
            exists. Only notified when previous client's lock is released so no
            need to retry except when you know it will work for sure
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p id="lecture_6_7" class="lecture_heading">
  Lecture 6-7: Fault Tolerance: Raft
</p>
<ul class="note_bullets">
  <li>
    Paper:
    <a href="https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf"
      >Raft (extended) (2014)</a
    >; <a href="https://www.youtube.com/watch?v=64Zp3tzNbpE">Lecture Part1</a>
    <a href="https://www.youtube.com/watch?v=4r8Mz3MMivY">Lecture Part2</a>
  </li>
  <li>Replicated State machine</li>
  <li>
    Must avoid Split Brain against network partition
    <ul>
      <li>Non-faulty networks: Communication failure → server down</li>
      <li>Human Intervention</li>
      <li>
        Majority Voting (sometimes called quorum systems): Odd number of servers
        <ul>
          <li>2F+1 servers → F failures and can still continue</li>
          <li>At most one paritition can have a majority of servers</li>
          <li>
            Any two majorities must overlap in at least one server (just count
            it! Pidgeonhole principle)
          </li>
          <li>After being elected, majority includes leader itself</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    Some similar systems
    <ul>
      <li>Paxos: Considered very challenging to understand</li>
      <li>Viewstamp Replication</li>
    </ul>
  </li>
  <li>
    Log: Leader gives an order for client operations so all clients see the same
    order
    <ul>
      <li>Persisted log allows for a server to recover after crashing</li>
      <li>
        Logs tails may diverge (i.e. leader crashes before sending them out to
        all the servers)
      </li>
    </ul>
  </li>
  <li>
    Leaderless systems exist, but require more messages (multiple rounds of
    voting) and are harder to reason about
  </li>
  <li>Each term has <b>at most</b> one leader</li>
  <li>
    If server has not heard from current leader after election timeout has run
    out, it will start an election
    <ul>
      <li>
        Increments term → Votes for itself → Generates a new election timeout
        interval randomly →Sends out RequestVotes RPC to all other servers
      </li>
      <li>
        If receieves votes from a majority of servers (including itself) →
        become leader
      </li>
      <li>
        Each server votes at most once per term (stored on disk to avoid
        forgetting)
      </li>
      <li>
        If AppendEntries RPC received from current leader (higher/equal term to
        itself), convert to Follower
      </li>
      <li>
        If election timeout elapses →start new election
        <ul>
          <li>
            Happens when vote is split up and not decisive or partitioned
            network
          </li>
          <li>
            Raft reduces the odds of split votes is by
            <b>randomizing</b> election timeouts so all servers do not try to
            start an election at the same exact time. This makes it so that one
            server will likely be able to send out a full round of vote requests
            before another server attempts to do the same. Other servers will
            all vote for the first server
          </li>
          <li>
            Average time of failures for a single server≫ Election timeout ≫
            heartbeat interval
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    Candidate after winning election sends out heartbeat to all other servers.
    Implicitly indicates they are the leader because there is at most one leader
    for a single term.
    <ul>
      <li>
        Heartbeats supress candidates from popping up because it resets election
        timeout of followers/candidates who receive messages
      </li>
    </ul>
  </li>
  <li>
    Who can be leader (how will servers decide whether to vote a certain other
    server as a leader or not)?
    <ul>
      <li>Cannot merely use longest log (in Raft)</li>
      <li>Must have term >= server's term</li>
      <li>
        Must have log at least as up to date as the server's own log
        <ul>
          <li>Compare last log entry</li>
          <li>Higher term log entry is more up to date</li>
          <li>If same term, larger log entry index is more up to date</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <b>Log Matching Property</b>
    <ol>
      <li>
        Log entries in two different logs with the same index and same term are
        the same log entry
      </li>
      <li>
        If a log entry is matching all preceding entries are also matching
      </li>
    </ol>
    <ul>
      <li>
        By default, the leader finds the last common log entry by decrementing
        its count of where the follower is by one and sending that index + term
        to the follower. The follower then checks to see if it has a log entry
        there with the same index + term (which then must be the same by the Log
        Matching Property)
        <ul>
          <li>This can be slow when a follower gets very far behind</li>
          <li>
            Alternate Scheme for higher performance when very far behind
            <ul>
              rejection from follower includes:
              <li>XTerm: term in the conflicting entry (if any)</li>
              <li>XIndex: index of first entry with that term (if any)</li>
              <li>XLen: log length</li>
            </ul>
            <ol>
              <li>
                Case 1 (leader doesn't have XTerm): nextIndex = XIndex (can
                reset to the last common term)
              </li>
              <li>
                Case 2 (leader has XTerm): nextIndex = leader's last entry for
                XTerm (i.e. when a follower has more log entries in a certain
                term than a leader, those entries should be deleted and thus go
                back to leader's last entry for XTerm)
              </li>
              <li>
                Case 3 (follower's log is too short): nextIndex = XLen (try to
                match at last entry of follower, and if needed go to case1/2)
              </li>
            </ol>
          </li>
          <li>
            Could also in theory use something like binary search, which might
            be faster
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    Persistence
    <ul>
      Relatively costly, so only persist the minimal number of things
      <li>
        currentTerm: Required to prevent two leaders from existing in the same
        term (due to forgetting about a previous term)
      </li>
      <li>
        votedFor: Required to prevent voting for multiple candidates in a single
        term
      </li>
      <li>
        log: Required for the information to reconstruct the application state
      </li>
      <li>
        Other things such as lastApplied do not need to be persisted as they can
        be reconstructed by looking at own log + responses from ApplyEntries.
        Leader will re-execute entries after a restart. Application might need
        logic to ignore duplicates
      </li>
    </ul>
  </li>
  <li>
    Log Compaction/Snapshots
    <ul>
      <li>
        Application is asked to save its state as of a particular log entry as
        state is generally smaller than log of changes
      </li>
      <li>Raft can then discard all earlier entries</li>
      <li>
        If earlier entries are needed for AppendEntries/restoring another
        server's state, use InstallSnapshot RPC
      </li>
    </ul>
  </li>
  <li>To be added: Membership changes</li>
</ul>

<p id="lecture_5" class="lecture_heading">Lecture 5: Go, Threads, and Raft</p>
<ul class="note_bullets">
  <li>add <i>-race</i> for go's race detection</li>
  <li>
    Closure for goroutines: Need to pass in changing variables explicitly from
    the outside go func(i int){ log.Print(i) }(i)
  </li>
  <li>
    SIGQUIT
    <ul>
      <li>
        the default SIGQUIT handler for Go prints stack traces for all
        goroutines (and then exits)
      </li>
      <li>Ctrl+\ will send SIGQUIT</li>
    </ul>
  </li>
  <li>Kill a single goroutine with <i>kill -QUIT pid</i></li>
</ul>

<p id="lecture_4" class="lecture_heading">
  Lecture 4: Primary-Backup Replication
</p>
<ul class="note_bullets">
  <li>
    Paper:
    <a href="https://pdos.csail.mit.edu/6.824/papers/vm-ft.pdf">
      Fault-Tolerant Virtual Machines (2010)</a
    >;
    <a href="https://www.youtube.com/watch?v=M_teob23ZzY">Lecture</a>
  </li>
  <li>
    Use deterministic replay of state changes to a single backup to allow for
    Fault tolerant VMs
  </li>
  <li>
    Main requirement: if the backup VM ever takes over after a failure of the
    primary, the backup VM will continue executing in a way that is entirely
    consistent with all outputs that the primary VM has sent to the external
    world.
    <ul>
      <li>
        Achieved by delaying output from primary until the backup has received
        the information needed to replay up to that output operation and has
        knowledged all entries up to that log's entry. Primary can continue
        executing, but cannot send output. Backup eventually replays all
        operations (discards output though)
      </li>
      <li>
        Non-deterministic actions are done on primary and the result is sent to
        the backup, which then uses the result
      </li>
      <li>
        Could consider a chain replication scheme instead of delaying output
        from the primary
      </li>
    </ul>
  </li>
  <li>
    Use heartbeats to determine if other server is alive
    <ul>
      <li>
        Must be careful as this can result in split brain if not careful when
        the backup think it's the primary and the primary thinks the backup is
        dead
      </li>
      <li>
        Split brain avoided by executing atomic test-and-set operation (returns
        true to only the first instance to try it) on shared storage. Operation
        succeeds → can go live. Fails →must abort. Storage acts as a tiebreaker
      </li>
      <li>Duplicate output can happen. TCP etc should be able to handle it</li>
    </ul>
  </li>
  <li>
    Alternatively, send over the entire state instead of state changes
    <ul>
      <li>
        This paper is only single core and has issues with how to switch to
        multicore
      </li>
      <li>
        Sending over entire state would make the problem much easier to handle
      </li>
    </ul>
  </li>
</ul>

<p id="lecture_3" class="lecture_heading">Lecture 3: GFS</p>
<ul class="note_bullets">
  <li>
    Paper:
    <a href="https://pdos.csail.mit.edu/6.824/papers/gfs.pdf">GFS (2003)</a>;
    <a href="https://www.youtube.com/watch?v=EpIgvowZr00">Lecture</a>
  </li>
  <li>
    Distributed file system built from many commodity servers to primarily
    handle sequential writes to large files
  </li>
  <li>
    Classic Issue: Performance → sharding over many machines → get many faults →
    need fault tolerance via replication → have potential for inconsistency →
    better consistency → low performance
  </li>
  <li>
    GFS
    <ul>
      <li>
        Many chunk servers that store actual data. One master keeps track of all
        metadata in two tables. Stored in both memory + disk.
        <ul>
          <li>File names mapped to array of chunk handles (non-volatile)</li>
          <li>
            Chunk Handles mapped to list of chunk servers (volatile, can ask all
            chunk servers what handles they have), version number
            (non-volatile), primary chunk server (volatile, just wait the chunk
            expiration), primary lease expiration time (volatile)
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    Operations
    <ul>
      <li>
        Read:
        <ul>
          <li>Client sends filename + offset in request to master</li>
          <li>
            Client gets list of chunk servers from master (cached by client)
          </li>
          <li>
            Client talks to one of the chunk servers (tries to pick closest) and
            gets the data back
          </li>
        </ul>
      </li>
      Write:
      <ul>
        <li>
          If no primary
          <ul>
            <li>
              finds chunk servers with the most up to date chunk (must match or
              be larger than the version number stored by the master), does not
              respond until it can figure this out.
            </li>
            <li>
              Picks a primary from the list of servers that have the most up to
              date chunk.
            </li>
            <li>
              Sends information to chunk servers of who is in charge + lease
              including new version number. When master writes version number to
              disk is unclear
            </li>
            <li>
              If master cannot communicate with primary, it can wait lease
              duration and then designate a new primary, avoiding split brain
            </li>
          </ul>
        </li>
        <li>
          Client sends data to all replicas (maybe just closest and then chain
          replication)
        </li>
        <li>
          Once all replicas have data, client tells primary to write the data
        </li>
        <li>
          Primary picks an offset, writes the data, and tells the replicas the
          offset and to write the data
        </li>
        <li>
          Replicas respond to primary and primary gets "yes" from all replicas,
          it responds to client that the operation has succeeded
        </li>
        <li>
          If replicas don't all respond "yes", primary responds no. Client
          should retry sequence from the start.
        </li>
        <li>
          Note that a single entry can appear multiple times or not at all due
          to retries and potentially different orders depending on the replica
          it talks to. Client must handle this
        </li>
      </ul>
    </ul>
  </li>
</ul>

<p id="lecture_2" class="lecture_heading">Lecture 2: RPC and Threads</p>
<ul class="note_bullets">
  <li>
    Threads: Share memory/cache. Can be very helpful, but must be careful about
    mutations.
    <ul>
      <li>Use locks to solve</li>
      <li>
        Change program to not share data (for increment common variables just
        increment separate variables and then increment all at the end)
      </li>
    </ul>
  </li>
  <li>
    Deadlocks: One thread holding a lock is waiting for another thread to
    release a lock. Unfortunately, other thread waiting for first thread to
    release the lock it is holding
  </li>
  <li>Channels: Pass messages to share data</li>
</ul>

<p id="lecture_1" class="lecture_heading">Lecture 1: Introduction</p>
<ul class="note_bullets">
  <li>
    Paper:
    <a href="https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdf"
      >MapReduce (2004)</a
    >;
    <a href="https://www.youtube.com/watch?v=cQP8WApzIQQ">Lecture</a>
  </li>
  <li>
    Distributed file system built from many commodity servers to primarily
    handle sequential writes to large files
  </li>
  <li>
    Distributed Systems
    <ul>
      <li>
        Set of cooperating computers communicating over network to do certain
        tasks
      </li>
      <li>
        Build for parallelism, fault tolerance, physical separation, and for
        security/isolation reasons
      </li>
      <li>
        Challenges: concurrency, partial failures (instead of full failure or
        full success), performance (hard to get linear scaling)
      </li>
    </ul>
  </li>
  <li>
    MapReduce
    <ul>
      <li>
        Goal for non-distributed system experts to be able to use distributed
        system infra to run large jobs
      </li>
      <li>
        Use "Map" to generate intermediate key/value pairs, and then "Reduce" to
        generate a final set of values
        <ul>
          <li>Map: (key1,value2) → (key2, value2)</li>
          <li>Reduce: (key2, list value2) → (key2, value3)</li>
        </ul>
      </li>
      <li>
        All Maps can run in parallel and so can all Reduces. N workers will give
        ~Nx performance
      </li>
      <li>
        Tasks are deterministic and repeatable. Many small tasks to avoid one
        task dominating the time
      </li>
      <li>Limitations: No state or real-time streaming processing</li>
    </ul>
  </li>
  <li>
    Fault Tolerance
    <ul>
      <li>
        Single master. Only fault tolerance is snapshotting and restarting from
        snapshot
      </li>
      <li>
        Master pings workers periodically, but if worker does not respond in a
        timely matter, the master marks the worker as failed and gives the tasks
        the worker was doing to another worker. Resilient to worker failure
      </li>
      <li>
        When map worker finishes job, sends master list of output files. Master
        records filenames. Reduce worker's finished results are handled via an
        atomic rename so multiple copies are not made
      </li>
    </ul>
  </li>
</ul>

<style>
  #toc_container {
    border: 1px solid #aaa;
    display: table;
    font-size: 85%;
    margin-bottom: 1em;
    padding: 10px;
    width: auto;
  }
  .toc_title {
    font-weight: 700;
    text-align: center;
  }
  #toc_container li,
  #toc_container ul,
  #toc_container ul li {
    list-style: circle outside none;
  }
  .lecture_heading {
    margin-bottom: 0;
    text-decoration: underline;
  }
  .note_bullets li,
  .note_bullets ul,
  .note_bullets {
    margin-bottom: 0;
  }
</style>
