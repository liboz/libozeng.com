<blurb>MIT concurrency notes</blurb>

<div id="toc_container">
  <p class="toc_title">Contents</p>
  <ul>
    <li>
      <a href="#lecture_14">Lecture 14: Optimistic Concurrency Control</a>
    </li>
    <li><a href="#lecture_13">Lecture 13: Spanner</a></li>
    <li><a href="#lecture_12">Lecture 12: Distributed Transactions</a></li>
    <li><a href="#lecture_11">Lecture 11: Cache Consistency: Frangipani</a></li>
    <li><a href="#lecture_10">Lecture 10: Cloud Replicated DB, Aurora</a></li>
    <li><a href="#lecture_9">Lecture 9: More Replication, CRAQ</a></li>
    <li><a href="#lecture_8">Lecture 8: Zookeeper</a></li>
  </ul>
</div>

<p id="lecture_14" class="lecture_heading">
  Lecture 14: Optimistic Concurrency Control
</p>
<ul class="note_bullets">
  <li>
    Paper:
    <a href="https://pdos.csail.mit.edu/6.824/papers/farm-2015.pdf"
      >FARM (2015)</a
    >; <a href="https://www.youtube.com/watch?v=Cw6Nj2evjSs">Lecture</a>
  </li>
  <li>
    Use remote direct memory access + non volatile memory (battery powered) to
    provide serializability (ordered transactions)
  </li>
  <li>
    Research prototype where the app uses special network interface cards (NICs)
    to read from memory of app on a different computer directly. Only viable in
    local networks at the moment, but crazy good performance (100x faster than
    spanner). Uses two phase commits
  </li>
  <li>
    Optimistic Concurrency Control by first reading the value directly from app
    memory on a different computer and then validating during the transaction
    after acquiring locks. Pessimistic Concurrency Control blocks on acquring
    locks while Optimistic aborts when it find that the valid it has read is no
    longer valid.
  </li>
  <li>Direct Read → Lock → Validate → Commit</li>
</ul>

<p id="lecture_13" class="lecture_heading">Lecture 13: Spanner</p>
<ul class="note_bullets">
  <li>
    Paper:
    <a href="https://pdos.csail.mit.edu/6.824/papers/spanner.pdf"
      >Spanner (2012)</a
    >; <a href="https://www.youtube.com/watch?v=4eW5SWBi7vs">Lecture</a>
  </li>
  <li>
    Use Paxos to make the transacation coordinator in two phase commits have
    fallbacks and so it does not completely block on transacation coordinator
    failure (availability). This allows for external
    consistency/serializability. High demand by programmers for transacations as
    easier to reason about
  </li>
  <li>
    Use versioning of objects (snapshot isolation) + time API to allow many read
    only transactions to complete by reading from the the local instance without
    locking. Paxos sends operations in timestamp order. Must wait until sees a
    Paxos operation with timestamp after the timestamp of the transaction (See
    start rule)
  </li>
  <li>
    Time API has uncertainty.
    <ul>
      <li>
        Start Rule: Transaction timestamp = time.now.latest.
        <ul>
          <li>Read only: start time</li>
          <li>Read write: commit time</li>
        </ul>
      </li>
      <li>
        Commit Wait: Wait to commit a read write transaction until Transaction
        timestamp < time.now.earliest
      </li>
    </ul>
  </li>
  <li>Used commerially by Google and also in Cockroachdb</li>
</ul>

<p id="lecture_12" class="lecture_heading">
  Lecture 12: Distributed Transactions
</p>
<ul class="note_bullets">
  <li>
    Paper:
    <a
      href="https://ocw.mit.edu/resources/res-6-004-principles-of-computer-system-design-an-introduction-spring-2009/online-textbook/"
    >
      Read 6.033 Chapter 9, just 9.1.5, 9.1.6, 9.5.2, 9.5.3, 9.6.3 </a
    >; <a href="https://www.youtube.com/watch?v=aDp99WDIM_4">Lecture</a>
  </li>
  <li>
    <b>Serializable</b>
    <ul>
      <li>
        There exists some serial order (one at a time) of execution of the
        transactions such that yields the same result as the actual execution
      </li>
      <li>
        Given two transactions T1, T2. The only possible serial orders are: T1,
        T2 or T2, T1. A serializable system must have the same result as running
        either T1 then T2 or T2 then T1.
      </li>
    </ul>
  </li>
  <li>
    Distributed Transactions with ACID:
    <ul>
      <li>
        concurrency control (to provide isolation/serializability)
        <ul>
          <li>
            Pessimistic: Acquire lock before using data. Wait if lock held by
            something else (faster when frequent conflicts)
          </li>
          <li>
            Optimistic: Don't acquire locks and just read/write to temporary
            area. At the end check if another system is using the data at the
            same time. If it was being used, abort. Faster when conflicts are
            rare
          </li>
        </ul>
      </li>
      <li>atomic commit (to provide atomicity despite failure)</li>
    </ul>
  </li>
  <li>
    Two phase locking (Pessimistic)
    <ul>
      <li>Acquire lock before using record</li>
      <li>
        Hold lock until completely done/abort (needed as opposed to release
        immediately after last usage of a record because an abort after that
        could result in the need to rollback the entire transaction)
      </li>
      <li>
        Easy to get deadlocks (db needs to smart to detect and abort
        transactions)
      </li>
    </ul>
  </li>
  <li>
    Two phase commits (used in many databases that are sharded + have
    multi-record transactions):
    <ul>
      <li>
        Phase One: Coordinator tells all participants the operations they need
        to perform. Participants prepare locks/rollback resources as needed.
        After receiving the response and about ready to commit, coordinator asks
        all participants if they can commit. If participants says yes, they must
        be remain in a state indefinitely where they can either do the operation
        or rollback.
      </li>
      <li>
        Phase Two:
        <ul>
          <li>
            Coordinator receives yes from all participants → Sends message to
            all participants to commit repeatedly until gets acknowledgement
            from all participants.
          </li>
          <li>
            Coordinator receives no from any participants → sends abort
            repeatedly to all participants to rollback until gets
            acknowledgement from all participants.
          </li>
        </ul>
      </li>
      <li>
        Has issues when coordinator fails as participants cannot change their
        locked without coordinator command to either proceed or abort. Generally
        requires manual intervention
      </li>
    </ul>
  </li>
</ul>

<p id="lecture_11" class="lecture_heading">
  Lecture 11: Cache Consistency: Frangipani
</p>
<ul class="note_bullets">
  <li>
    Paper:
    <a href="https://pdos.csail.mit.edu/6.824/papers/thekkath-frangipani.pdf"
      >Frangipani</a
    >; <a href="https://www.youtube.com/watch?v=-pKNCjUhPjQ">Lecture</a>
  </li>
  <li>
    Networked file system with locking to guarantee cache coherence/atomicity
    using Petal (block storage service)
  </li>
  <li>
    Cache Coherence Rules
    <ul>
      <li>
        No workstation can hold cached data without holding a lock (either busy
        using it or idling holding it)
        <ul>
          <li>Acquire Lock by requesting from Petal → Read from Petal</li>
          <li>
            When planning to release lock → first write cached data to Petal
          </li>
        </ul>
      </li>
      <li>
        Don't give up lock after finishing the usage of data (lazy about handing
        locks back). Typical case where you creating files/directories and use
        them immediately is made fast
      </li>
      <li>
        Data is in RAM typically. Anything in cache is written to file every ~30
        seconds
      </li>
      <li>
        Lock server keeps track of locks for specific files and the workstation
        that owns that lock
      </li>
      <li>
        Example:
        <ul>
          <li>
            Workstation 1: Request lock to lock server → receive Grant lock from
            lock server
          </li>
          <li>
            Workstation 1: Continues to hold lock even when not using it
            directly
          </li>
          <li>
            Workstation 2: Wants the lock so sents a Request lock to lock server
          </li>
          <li>
            Lock Server: Checks table and sees lock owned by Workstation 1 so
            sends Revoke to workstation 1
          </li>
          <li>
            Workstation 1: Writes any cached data to write ahead log and then
            Petal → Release lock message to Lock Server
          </li>
          <li>
            Lock Server: receives Release lock from Workstation 1 → Sends Grant
            lock to Workstation 2
          </li>
          <li>Workstation 2: Uses file</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    Crash Recovery
    <ul>
      <li>
        A workstation must write to its own write ahead log in Petal (allows
        other workstations to acccess it in case of crashes) before beginning
        any write that it does
      </li>
      <li>
        Log Entries will have a version number with version before write + 1
      </li>
      <li>
        On crash, other computer checks log and knows the last one by
        monotonically increasing log sequence number. Each entry has description
        of file operations
        <ul>
          <li>
            Replays the log entries into Petal and then tells Petal to release
            the locks
          </li>
          <li>Only plays complete log entries (no partial writes)</li>
          <li>
            Ignores log entries that have version numbers <= version number in
            Petal directly (only replays newer entries)
          </li>
          <li>
            Recovery reads without acquiring locks even though other systems may
            have the locks
            <ul>
              <li>
                Cannot participate in locking because if power outage all
                knowledge about locks could be lost and then the recovery
                software cannot continue
              </li>
              <li>
                This is okay because only 2 possibilities
                <ul>
                  <li>
                    workstation had lock when crashed → nothing else could have
                    written to it as lock was not acquired by any other system
                    so we are safe. version number < version in log
                  </li>
                  <li>
                    workstation gave up lock when crashed → must have already
                    written to Petal so version number will be >= version in log
                    and recovery does nothing
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p id="lecture_10" class="lecture_heading">
  Lecture 10: Cloud Replicated DB, Aurora
</p>
<ul class="note_bullets">
  <li>
    Paper:
    <a href="https://pdos.csail.mit.edu/6.824/papers/aurora.pdf"
      >Aurora (2017)</a
    >; <a href="https://www.youtube.com/watch?v=jJSh54J1s5o">Lecture</a>
  </li>
  <li>
    Cloud based database on AWS with quorum based reads/writes across multiple
    availability zones
  </li>
  <li>
    Quorums
    <ul>
      <li>
        Consistency achived when <i>Vr + Vw > V</i>, where <i>Vr</i> is the
        number of votes needed for a read request, <i>Vw</i> is the number of
        votes needed for a write request and <i>V</i> is the number of copies
      </li>
      <li>
        Aurora has a goal of having read availability when one availability zone
        + 1 server is down. They achieve this by having 6 copies across 3
        availability zones. Then a single zone + 1 server can be down and a read
        quorum of 3 can be achieved. Write quorum must then be 4
      </li>
    </ul>
  </li>
  <li>
    Only stores "redo" logs as opposed to pages. Materializes pages when needed.
    The log is the database
    <ul>
      <li>
        Writes through a single database, many read only databases
        <ul>
          <li>
            Read only databases can directly interact with storage servers to
            grab data. Also cache data, but lag to some extent
          </li>
          <li>
            mini transactions/VDL numbers etc are used to ensure consistency for
            the read only databases
          </li>
        </ul>
      </li>
      <li>
        Aurora typically does not use quorum reads as it keeps track of how far
        in the log each storage server has reached and sends read requests to a
        storage server whose log is up to date enough. Quorum reads used in
        recovery
        <ul>
          <li>
            Quorum reads are used in recovery to find the first missing log
            entry amongst the quorumed servers and discards all entries
            following that
          </li>
          <li>
            Only log entries of uncommitted entries are discarded due to the
            quorum write property
          </li>
        </ul>
      </li>
      <li>
        Redo log:
        <ul>
          <li>Old value + new value</li>
          <li>
            Commit record (existence of this indicates if the transaction has
            completed or not)
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    Network is the main bottleneck, dealt with via customized storage servers
  </li>
</ul>

<p id="lecture_9" class="lecture_heading">Lecture 9: More Replication, CRAQ</p>
<ul class="note_bullets">
  <li>
    Paper:
    <a href="https://pdos.csail.mit.edu/6.824/papers/craq.pdf">CRAQ (2009)</a>;
    <a href="https://www.youtube.com/watch?v=IXHzbCuADt0">Lecture</a> (majority
    of the lecture is about Zookeeper, CRAQ part starts at ~57:20)
  </li>
  <li>
    Improves on Chain Replication with higher read throughput that allows gives
    applications the option for weaker consistency guarantees on non-committed
    operations
  </li>
  <li>
    Chain Replication
    <ul>
      <li>Linearly ordered sequence of servers</li>
      <li>All writes are processed by head node</li>
      <li>
        Head node sends data to next node, which then sends data to the node
        after it. Repeat until reaches tail node
      </li>
      <li>
        Tail node sends acknowledgement that it has committed, which then
        propagates via the chain back to the head. Then head can then respond to
        the client that the write operation has completed
      </li>
      <li>All reads are processed by tail node</li>
      <li>Strong consistency</li>
    </ul>
  </li>
  <li>
    Chain Replication with Apportioned Queries (CRAQ)
    <ul>
      <li>
        Each node stores multiple version of an object
        <ul>
          <li>Has monotonically increasing version number</li>
          <li>Also has clean/dirty flag</li>
        </ul>
      </li>
      <li>
        When a node receives an update from a node higher up in the chain (or
        from client), appends that update to the object with an increased
        version number
        <ul>
          <li>
            Not last node → mark as dirty and propagate to node proceeding
            itself
          </li>
          <li>
            Last node → mark as clean and sends acknowledgement up the chain
          </li>
        </ul>
      </li>
      <li>
        node receives an acknowledgement message for an object → mark object as
        clean and can delete non-latest version of object
      </li>
      <li>
        node receives a read request for an object
        <ul>
          <li>
            Latest version is clean (aka has one version of the object) →
            responds with the object. The tail is guaranteed not to have seen
            the object yet
          </li>
          <li>
            Latest version is dirty → sends message to tail to get the tail's
            last commit number of the object and responds with that version of
            the object (which must exist)
          </li>
          <li>
            It is possible tail commits a new version between when it respond to
            the node and when the node sends to the client. But this is still
            strongly consistent because consistency is ordered to when tail
            responded
          </li>
        </ul>
      </li>
      <li>
        Can use as eventual consistency system by having nodes respond to read
        requests by serving the latest version available. A request to a
        different node may result in an older version of the object
      </li>
    </ul>
  </li>
  <li>
    Faster than a system like Raft/Paxos as reads/writes are divided amongst
    multiple servers
    <ul>
      <li>
        Handles lagging servers poorly as a single lagging server would slow all
        writes down
      </li>
    </ul>
  </li>
  <li>
    Failure recovery is similar to Chain Replication in that every node knows
    its predecessor and successor and can generally replace it with no extra
    work (nodes internal in the chain failing may require resends)
    <ul>
      <li>
        Not resistant to partition failure. General way to deal with this is a
        configuration manager such as Zookeeper (or similar Raft/Paxos
        replicated system) to keep track of which servers are alive and the
        current setup of the chain
      </li>
    </ul>
  </li>
</ul>

<p id="lecture_8" class="lecture_heading">Lecture 8: Zookeeper</p>
<ul class="note_bullets">
  <li>
    Paper:
    <a href="https://pdos.csail.mit.edu/6.824/papers/zookeeper.pdf"
      >ZooKeeper (2010)</a
    >;
    <a href="https://www.youtube.com/watch?v=pbmyrNjzdDk">Lecture Part1</a>
    <a href="https://www.youtube.com/watch?v=IXHzbCuADt0">Lecture Part2</a>
    (until ~57:20)
  </li>
  <li>
    Stand alone service used for coordination of other services. Uses ZAB, which
    is similar to Raft/Paxos
  </li>
  <li>
    Linearizability
    <ul>
      <li>
        A history is linearizable if there exists a total order of operations
        that matches real time and reads see proceeding writes in the order
      </li>
      <li>Cycles =/= not linearizable</li>
      <li>Zookeeper does not use the standard definition of linearizability</li>
      <li>
        Zookeeper alows reads to return stale data (unless using <i>sync</i>)
      </li>
    </ul>
  </li>
  <li>
    Zookeeper Guarantees
    <ul>
      <li>Writes are linearizable</li>
      <li>
        For any given client, Zookeeper executes operations in a client
        specified order (FIFO)
        <ul>
          <li>
            Writes: Client gives an ordering of the writes for the leader to
            execute
          </li>
          <li>
            Reads: A read has a given effective location in the log. Any future
            read must be at least as up to date as that previous read. This
            guarantee holds even when switching to a different replica
            <ul>
              <li>
                When replica responds to a client read request, replica attaches
                zxID of the previous entry to the response. Client remembers the
                most recent zxID it sees. Client sends that id to future
                requests
              </li>
              <li>
                Also applies to reads followed by writes. Read your own writes
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <i>sync</i> + FIFO client order allows one to see the most recent data
        if needed
      </li>
    </ul>
  </li>
  <li>
    Strategies
    <ul>
      <li>
        Ready file (acts like a lock): before modifying configuration, delete
        "ready" file. Then make changes and at the end recreate "ready" file.
        This allows other systems to know that the configuration is in a good
        state
      </li>
      <li>
        When trying to read configurations, read for "ready" file existence (to
        know the configuration is good) with a watch to be notified of any
        change (and thus know the configuration is stale). Watches not carried
        over by replicate failure so need to restart with watches when replicate
        crashes (client is notified of this)
      </li>
      <li>
        Most operations have version number so zookeeper only does an operation
        when the current version number = version sent via request
      </li>
      <li>
        while true: getData(id) -> if setData(id, new_value ,version + 1): break
        <ul>
          <li>mini-transactions</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    Thundering herd
    <ul>
      <li>
        Because typically one uses version numbers to prevent running with stale
        data, it is common to implment with a loop. Many many clients trying at
        the same time will result in a thundering herd
        <ul>
          <li>
            General solution: Randomized Exponentially increasing delays between
            retries
          </li>
          <li>
            Ephemeral nodes: Tries to create ephemeral, but if it fails check if
            exists with a watch to know when the lock is released
          </li>
          <li>
            Sequential file: Check if previously number file in the sequence
            exists. Only notified when previous client's lock is released so no
            need to retry except when you know it will work for sure
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p id="lecture_6_7" class="lecture_heading">
  Lecture 6-7: Fault Tolerance: Raft
</p>
<ul class="note_bullets">
  <li>
    Paper:
    <a href="https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf"
      >Raft (extended) (2014)</a
    >; <a href="https://www.youtube.com/watch?v=64Zp3tzNbpE">Lecture Part1</a>
    <a href="https://www.youtube.com/watch?v=4r8Mz3MMivY">Lecture Part2</a>
  </li>
  <li>Replicated State machine</li>
  <li></li>
  <li></li>
  <li></li>
</ul>

<style>
  #toc_container {
    border: 1px solid #aaa;
    display: table;
    font-size: 85%;
    margin-bottom: 1em;
    padding: 10px;
    width: auto;
  }
  .toc_title {
    font-weight: 700;
    text-align: center;
  }
  #toc_container li,
  #toc_container ul,
  #toc_container ul li {
    list-style: circle outside none;
  }
  .lecture_heading {
    margin-bottom: 0;
    text-decoration: underline;
  }
  .note_bullets li,
  .note_bullets ul,
  .note_bullets {
    margin-bottom: 0;
  }
</style>
